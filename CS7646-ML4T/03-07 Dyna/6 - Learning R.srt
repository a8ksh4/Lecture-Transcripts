1
00:00:00,470 --> 00:00:05,129
The last step here is how
do we learn a model for R?

2
00:00:05,129 --> 00:00:09,639
Now remember,
when we execute an action a in state s,

3
00:00:09,639 --> 00:00:13,758
we get an immediate reward, little r.

4
00:00:13,758 --> 00:00:16,500
So again, big R, s,

5
00:00:16,500 --> 00:00:21,850
a as are expected reward if we're
in state s and we execute action a.

6
00:00:21,850 --> 00:00:27,660
Little r is our immediate reward when
we experience this in the real world.

7
00:00:27,660 --> 00:00:32,650
So big R is our model, little r is
what we get in an experience tuple.

8
00:00:32,650 --> 00:00:39,070
So we want to update this model every
time we have a real experience.

9
00:00:39,070 --> 00:00:45,520
And it's a simple equation, very much
like the q table update equation.

10
00:00:45,520 --> 00:00:50,887
What we have is one minus alpha
where alpha is our learning rate and

11
00:00:50,887 --> 00:00:55,780
again that can typically be
something like zero point two.

12
00:00:55,780 --> 00:01:01,810
Anyways we multiply that times
our current value for R and

13
00:01:01,810 --> 00:01:08,130
then we add in of course our new
estimate of what that value ought to be.

14
00:01:08,130 --> 00:01:10,760
And we just use r for the new estimate.

15
00:01:10,760 --> 00:01:14,310
So it's alpha times little r,
which is our immediate reward, or

16
00:01:14,310 --> 00:01:17,230
our new best estimate of
what the value should be,

17
00:01:17,230 --> 00:01:23,000
plus what the value was
before times 1 minus alpha.

18
00:01:23,000 --> 00:01:26,220
So we're waiting presumably.

19
00:01:26,220 --> 00:01:32,050
Our old value more than our new value,
so we converge more slowly.

20
00:01:32,050 --> 00:01:32,720
But that's it.

21
00:01:32,720 --> 00:01:35,770
That's a simple way
to build a model of R

22
00:01:35,770 --> 00:01:38,370
from observations of interactions
with the real world.

