1
00:00:00,100 --> 00:00:04,100
I've mentioned overfitting before,
but I haven't yet defined it.

2
00:00:04,100 --> 00:00:05,580
Before we could define it, and

3
00:00:05,580 --> 00:00:09,800
I could give you an example,
we needed to have a definition of error.

4
00:00:09,800 --> 00:00:12,050
Let me now show you what I mean.

5
00:00:12,050 --> 00:00:17,480
Let's consider parameterized polynomial
models where we can, one at a time, add

6
00:00:17,480 --> 00:00:22,320
additional factors, like x, x squared,
x cubed, x to the fourth, and so on.

7
00:00:22,320 --> 00:00:26,120
Let's create a graph where we
have along the horizontal access

8
00:00:26,120 --> 00:00:31,470
degrees of freedom, or d,
the degree of our polynomial.

9
00:00:31,470 --> 00:00:35,110
And vertically here,
we'll have the error of our model.

10
00:00:35,110 --> 00:00:41,050
So let's measure error as we
increase d on our training set.

11
00:00:41,050 --> 00:00:46,510
So when d is smallest,
our error is greatest.

12
00:00:46,510 --> 00:00:51,890
And as we increase d,
our error drops and drops and drops.

13
00:00:51,890 --> 00:00:56,040
In other words, we're fitting
the data in sample better and better.

14
00:00:56,040 --> 00:01:01,680
When finally we get to N, where we
have as many parameters in our model

15
00:01:01,680 --> 00:01:06,210
as we do have items in our data set,
our error gets all the way down to zero.

16
00:01:06,210 --> 00:01:09,060
This is in sample error.

17
00:01:09,060 --> 00:01:12,460
Now, let's add a similar line for
out of sample error.

18
00:01:12,460 --> 00:01:15,410
Remember that we expect our
out of sample error to always

19
00:01:15,410 --> 00:01:18,460
be greater than or
equal to in sample error.

20
00:01:18,460 --> 00:01:20,740
The curve will look something like this.

21
00:01:20,740 --> 00:01:26,080
It'll start out at maximum error, about
the same as our in sample line, and

22
00:01:26,080 --> 00:01:30,190
as we go down,
we begin to diverge like this.

23
00:01:30,190 --> 00:01:33,710
Now in this region
both our in sample and

24
00:01:33,710 --> 00:01:37,390
out of sample errors
are still decreasing, but

25
00:01:37,390 --> 00:01:42,430
eventually we'll reach a point where
our out of sample begins to increase.

26
00:01:42,430 --> 00:01:45,770
In fact it may increase strongly.

27
00:01:45,770 --> 00:01:52,020
In this area, as we increase degrees
of freedom, our in sample error is

28
00:01:52,020 --> 00:01:57,330
decreasing, but
our out of sample error is increasing.

29
00:01:57,330 --> 00:01:59,700
And that's how we define overfitting.

30
00:01:59,700 --> 00:02:02,530
This is the region where
overfitting is occurring.

31
00:02:02,530 --> 00:02:04,430
So, let me state that again.

32
00:02:04,430 --> 00:02:09,949
In sample error is decreasing,
out of sample error is increasing.

33
00:02:09,949 --> 00:02:13,170
And we have those two together,
it's over fitting.

