1
00:00:00,340 --> 00:00:04,010
Q learning is named
after the Q function.

2
00:00:05,120 --> 00:00:06,430
What is that?

3
00:00:06,430 --> 00:00:08,972
Well, let's dig in and find out.

4
00:00:08,972 --> 00:00:13,968
Q can be written as a function, so
we might have parentheses around s and

5
00:00:13,968 --> 00:00:16,950
a, or you can think of it as a table.

6
00:00:16,950 --> 00:00:20,710
So in this class we're
going to view Q as a table.

7
00:00:20,710 --> 00:00:24,190
And it's got two dimensions, s and a.

8
00:00:24,190 --> 00:00:31,580
So s is the state that we're looking at,
and a is the action we might take.

9
00:00:31,580 --> 00:00:35,510
Q represents the value of taking action

10
00:00:35,510 --> 00:00:41,080
a in state s, and
there's two components to that.

11
00:00:41,080 --> 00:00:45,730
The two components are the immediate
reward that you get for

12
00:00:45,730 --> 00:00:50,201
taking action a in state s,
plus the discounted reward.

13
00:00:50,201 --> 00:00:53,945
And what that is about,
what the discounted reward is about,

14
00:00:53,945 --> 00:00:57,130
is the reward you get for
future actions.

15
00:00:57,130 --> 00:01:02,150
So an important thing to know
is that Q is not greedy,

16
00:01:02,150 --> 00:01:06,620
in the sense that it just represents
the reward you get for acting now.

17
00:01:06,620 --> 00:01:11,180
It also represents the reward you
get for acting in the future.

18
00:01:11,180 --> 00:01:14,970
Let's suppose we have Q
already created for us.

19
00:01:14,970 --> 00:01:16,520
We have this table.

20
00:01:16,520 --> 00:01:18,940
How can we use it to
figure out what to do?

21
00:01:20,010 --> 00:01:24,190
So what we do in any particular
state is the policy.

22
00:01:24,190 --> 00:01:27,900
And we represent the policy with pi.

23
00:01:27,900 --> 00:01:34,000
So pi of s means, what is the action
we take when we are in state s,

24
00:01:34,000 --> 00:01:37,810
or what is the policy for state s?

25
00:01:37,810 --> 00:01:41,480
And we take advantage of our
Q table to figure that out.

26
00:01:41,480 --> 00:01:42,880
Here's how it works.

27
00:01:42,880 --> 00:01:47,950
We're in state s and we want to
find out which action is the best.

28
00:01:47,950 --> 00:01:52,340
Well, all we need to do is look
across all the potential actions and

29
00:01:52,340 --> 00:01:58,840
find out which value of
(Q [s,a]) is maximized.

30
00:01:58,840 --> 00:02:02,820
So we don't change s, we just
step through each value of a, and

31
00:02:02,820 --> 00:02:06,710
the one that is the largest
is the action we should take.

32
00:02:06,710 --> 00:02:12,386
And the mathematical way to represent
this is to use the function argmax,

33
00:02:12,386 --> 00:02:17,210
so argmax of a of this function.

34
00:02:17,210 --> 00:02:21,190
So what that does is it finds
the a that maximizes this, and

35
00:02:21,190 --> 00:02:22,430
then the answer is a.

36
00:02:23,580 --> 00:02:25,930
After we run Q learning for long enough,

37
00:02:25,930 --> 00:02:29,430
we will eventually converge
to the optimal policy.

38
00:02:29,430 --> 00:02:32,870
And we represent that
with a little star.

39
00:02:32,870 --> 00:02:35,863
So the optimal policy is pi star of s.

40
00:02:35,863 --> 00:02:44,370
[COUGH] And similarly the optimal
Q table is Q star [s, a].

41
00:02:44,370 --> 00:02:47,249
Now this is how to use
a Q table if you have it.

42
00:02:48,380 --> 00:02:52,090
We need now to consider how do we
build that Q table in the first place.

